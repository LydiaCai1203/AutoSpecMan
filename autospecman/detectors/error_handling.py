"""Error handling detection using CodeIndex semantic search."""

from __future__ import annotations

import json
import os
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    from codeindex_sdk import CodeIndexClient, CodeIndexConfig
    HAS_CODEINDEX = True
    CODEINDEX_IMPORT_ERROR = None
except ImportError as e:
    HAS_CODEINDEX = False
    CODEINDEX_IMPORT_ERROR = str(e)

from ..llm import LLMAnalyzer, create_analyzer
from ..config import load_config


def detect_error_handling(
    repo_path: Path,
    use_llm: bool = True,
    llm_provider: str = "openai",
    llm_model: str = "gpt-3.5-turbo",
    llm_api_key: Optional[str] = None,
    llm_api_base_url: Optional[str] = None,
    codeindex_db_path: Optional[str] = None,
) -> Dict[str, Any]:
    """Detect error handling patterns using CodeIndex semantic search.
    
    Args:
        repo_path: Path to repository root
        use_llm: Whether to use LLM for analysis
        llm_provider: LLM provider name
        llm_model: LLM model name
        llm_api_key: LLM API key
        llm_api_base_url: LLM API base URL
        codeindex_db_path: Optional path to CodeIndex database file
        
    Returns:
        Dictionary with error_handling information
    """
    print("\n" + "="*60)
    print("ğŸ” å¼€å§‹é”™è¯¯å¤„ç†æœºåˆ¶åˆ†æ")
    print("="*60)
    
    result = {
        "error_handling_approach": None,
        "error_handling_details": None,
        "confidence": 0.0,
    }
    
    if not HAS_CODEINDEX:
        print("âŒ CodeIndex SDK æœªå®‰è£…ï¼Œè·³è¿‡é”™è¯¯å¤„ç†æ£€æµ‹")
        print("   å¯¼å…¥é”™è¯¯è¯¦æƒ…:")
        if CODEINDEX_IMPORT_ERROR:
            print(f"     {CODEINDEX_IMPORT_ERROR}")
        else:
            print("     æ— æ³•å¯¼å…¥ codeindex_sdk æ¨¡å—")
        print("\n   å®‰è£…æ–¹æ³•:")
        print("   1. è¿›å…¥ ast-demo/sdk/python ç›®å½•")
        print("   2. è¿è¡Œ: pip install -e .")
        print("   3. æˆ–è€…: pip install -e ../ast-demo/sdk/python")
        print("   4. ç¡®ä¿åœ¨æ­£ç¡®çš„è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…ï¼ˆå½“å‰ä½¿ç”¨çš„æ˜¯ .venvï¼‰")
        print("="*60)
        return result
    
    print("âœ… CodeIndex SDK å·²å®‰è£…")
    
    # Try to find codeindex database
    if codeindex_db_path:
        codeindex_db = Path(codeindex_db_path)
        # If relative path, resolve relative to repo_path
        if not codeindex_db.is_absolute():
            codeindex_db = repo_path / codeindex_db
        if not codeindex_db.exists():
            print(f"âŒ æŒ‡å®šçš„ CodeIndex æ•°æ®åº“ä¸å­˜åœ¨: {codeindex_db}")
            print("="*60)
            return result
        print(f"âœ… æ‰¾åˆ° CodeIndex æ•°æ®åº“: {codeindex_db}")
    else:
        print("ğŸ“‚ æ­£åœ¨æŸ¥æ‰¾ CodeIndex æ•°æ®åº“...")
        codeindex_db = _find_codeindex_db(repo_path)
        if not codeindex_db:
            print(f"âŒ æœªæ‰¾åˆ° CodeIndex æ•°æ®åº“")
            print(f"   å·²æœç´¢ä½ç½®:")
            print(f"     - {repo_path / '.codeindex' / 'project.db'}")
            print(f"     - {repo_path / '.codeindex' / 'sqlite.db'}")
            print(f"     - {repo_path / '.codeindex' / f'{repo_path.name}.db'}")
            print("   æç¤º: è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® codeindex.db_path æˆ–ç¡®ä¿æ•°æ®åº“å­˜åœ¨äºä¸Šè¿°ä½ç½®")
            print("="*60)
            return result
        print(f"âœ… è‡ªåŠ¨æ‰¾åˆ° CodeIndex æ•°æ®åº“: {codeindex_db}")
    
    # Try to detect languages from the repository
    print("\nğŸ“ æ£€æµ‹ç¼–ç¨‹è¯­è¨€...")
    languages = _detect_languages_from_repo(repo_path)
    if not languages:
        print("âŒ æœªèƒ½æ£€æµ‹åˆ°ç¼–ç¨‹è¯­è¨€")
        print("="*60)
        return result
    
    print(f"âœ… æ£€æµ‹åˆ°ç¼–ç¨‹è¯­è¨€: {', '.join(languages)}")
    
    # Search for error handling related code
    print("\nğŸ” å¼€å§‹è¯­ä¹‰æœç´¢é”™è¯¯å¤„ç†ç›¸å…³ä»£ç ...")
    # Load config from file to get LLM settings
    config = load_config(repo_path)
    code_snippets = _search_error_handling_code(
        repo_path, codeindex_db, languages,
        llm_api_key=llm_api_key or config.llm.api_key,
        llm_api_base_url=llm_api_base_url or config.llm.api_base_url,
        embedding_model=config.llm.embedding_model,
    )
    
    if not code_snippets:
        print("âŒ æœªæ‰¾åˆ°é”™è¯¯å¤„ç†ç›¸å…³çš„ä»£ç ç‰‡æ®µ")
        print("   å¯èƒ½åŸå› :")
        print("   1. Embedding API æœªé…ç½®ï¼ˆè¯·åœ¨ autospecman.toml ä¸­è®¾ç½® [llm].api_key å’Œ [llm].api_base_urlï¼‰")
        print("   2. ä»£ç åº“ä¸­ç¡®å®æ²¡æœ‰é”™è¯¯å¤„ç†ç›¸å…³çš„ä»£ç ")
        print("   3. CodeIndex æ•°æ®åº“æœªåŒ…å«ç›¸å…³ä»£ç çš„ç´¢å¼•")
        print("="*60)
        return result
    
    print(f"âœ… æ‰¾åˆ° {len(code_snippets)} ä¸ªç›¸å…³ä»£ç ç‰‡æ®µ:")
    for i, snippet in enumerate(code_snippets[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  {i}. {snippet['symbol_name']} ({snippet['symbol_kind']})")
        print(f"     æ–‡ä»¶: {snippet['file_path']}:{snippet['line_number']}")
        print(f"     ç›¸ä¼¼åº¦: {snippet['similarity']:.2f}")
        if snippet.get('summary'):
            summary = snippet['summary'][:100]
            print(f"     æ‘˜è¦: {summary}...")
    if len(code_snippets) > 10:
        print(f"  ... è¿˜æœ‰ {len(code_snippets) - 10} ä¸ªç»“æœ")
    
    # Analyze code snippets with LLM if available
    if use_llm:
        print(f"\nğŸ¤– ä½¿ç”¨ LLM åˆ†æé”™è¯¯å¤„ç†æ¨¡å¼ (æ¨¡å‹: {llm_model})...")
        analyzer = create_analyzer(
            provider=llm_provider,
            model=llm_model,
            api_key=llm_api_key,
            api_base_url=llm_api_base_url,
        )
        if analyzer:
            print("âœ… LLM åˆ†æå™¨åˆ›å»ºæˆåŠŸ")
            analysis = _analyze_error_handling_with_llm(analyzer, code_snippets)
            if analysis:
                result["error_handling_approach"] = analysis.get("approach")
                result["error_handling_details"] = analysis.get("details")
                result["confidence"] = 0.7  # High confidence when LLM analysis succeeds
                print("âœ… LLM åˆ†æå®Œæˆ")
                print(f"   å¤„ç†æ–¹å¼: {result['error_handling_approach']}")
                if result['error_handling_details']:
                    details = result['error_handling_details']
                    if len(details) > 200:
                        print(f"   è¯¦ç»†ä¿¡æ¯: {details[:200]}...")
                    else:
                        print(f"   è¯¦ç»†ä¿¡æ¯: {details}")
                else:
                    print("   è¯¦ç»†ä¿¡æ¯: æ— ")
                print("="*60)
                return result
            else:
                print("âŒ LLM åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†æ")
                print("   æç¤º: å¯èƒ½æ˜¯ API æœåŠ¡ä¸å¯ç”¨ã€æ¨¡å‹åç§°ä¸æ­£ç¡®æˆ–é…é¢å·²ç”¨å®Œ")
                print(f"   ä½¿ç”¨çš„æ¨¡å‹: {llm_model}")
                print(f"   API ç«¯ç‚¹: {llm_api_base_url or 'æœªè®¾ç½®'}")
        else:
            print("âŒ æ— æ³•åˆ›å»º LLM åˆ†æå™¨ï¼Œå›é€€åˆ°åŸºç¡€åˆ†æ")
            print("   æç¤º: è¯·æ£€æŸ¥ LLM API é…ç½®ï¼ˆapi_key, api_base_urlï¼‰")
    else:
        print("\nâš ï¸  LLM åˆ†æå·²ç¦ç”¨ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
    
    # Fallback: basic analysis without LLM
    print("\nğŸ“Š æ‰§è¡ŒåŸºç¡€é”™è¯¯å¤„ç†åˆ†æ...")
    result["error_handling_approach"] = _basic_error_handling_analysis(code_snippets)
    result["confidence"] = 0.4  # Lower confidence without LLM
    print(f"âœ… åŸºç¡€åˆ†æå®Œæˆ: {result['error_handling_approach']}")
    print("="*60)
    return result


def _find_codeindex_db(repo_path: Path) -> Optional[Path]:
    """Find codeindex database in the repository."""
    # Common locations for codeindex database
    possible_paths = [
        repo_path / ".codeindex" / "project.db",
        repo_path / ".codeindex" / "sqlite.db",
        repo_path / ".codeindex" / f"{repo_path.name}.db",
    ]
    
    for db_path in possible_paths:
        if db_path.exists():
            return db_path
    
    return None


def _detect_languages_from_repo(repo_path: Path) -> List[str]:
    """Detect programming languages from repository structure."""
    languages = []
    
    # Check for common language indicators
    if (repo_path / "go.mod").exists():
        languages.append("go")
    if (repo_path / "package.json").exists():
        languages.append("ts")
        languages.append("js")
    if (repo_path / "pyproject.toml").exists() or (repo_path / "requirements.txt").exists():
        languages.append("python")
    if (repo_path / "Cargo.toml").exists():
        languages.append("rust")
    if (repo_path / "pom.xml").exists() or (repo_path / "build.gradle").exists():
        languages.append("java")
    
    return languages if languages else ["ts", "js", "python", "go"]  # Default fallback


def _search_error_handling_code(
    repo_path: Path,
    db_path: Path,
    languages: List[str],
    llm_api_key: Optional[str] = None,
    llm_api_base_url: Optional[str] = None,
    embedding_model: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """Search for error handling related code using CodeIndex semantic search."""
    code_snippets = []
    
    # Error handling related queries (both Chinese and English)
    queries = [
        "é”™è¯¯å¤„ç†æœºåˆ¶",
        "error handling",
        "å¼‚å¸¸å¤„ç†",
        "exception handling",
        "é”™è¯¯å“åº”æ ¼å¼",
        "error response format",
        "è‡ªå®šä¹‰å¼‚å¸¸ç±»",
        "custom exception classes",
        "é”™è¯¯ç å®šä¹‰",
        "error code definitions",
    ]
    
    print(f"   æœç´¢æŸ¥è¯¢: {len(queries)} ä¸ªå…³é”®è¯")
    
    try:
        # Get embedding configuration from parameters, environment or config
        embedding_config = _get_embedding_config(
            llm_api_key=llm_api_key,
            llm_api_base_url=llm_api_base_url,
            embedding_model=embedding_model,
        )
        if not embedding_config.get("apiKey") or not embedding_config.get("apiEndpoint"):
            print("   âŒ æœªé…ç½® Embedding API")
            print("      éœ€è¦è®¾ç½®ä»¥ä¸‹ä¹‹ä¸€ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰:")
            print("      1. CLI å‚æ•°: --llm-api-key å’Œ --llm-api-base-url")
            print("      2. é…ç½®æ–‡ä»¶ autospecman.toml ä¸­çš„ [llm] éƒ¨åˆ†:")
            print("         api_key = \"your-api-key\"")
            print("         api_base_url = \"https://api.example.com/v1\"")
            print("      3. ç¯å¢ƒå˜é‡: OPENAI_API_KEY æˆ– LLM_API_KEY")
            return code_snippets
        
        print(f"   âœ… Embedding é…ç½®å·²å°±ç»ª")
        print(f"      æ¨¡å‹: {embedding_config.get('model', 'N/A')}")
        print(f"      API ç«¯ç‚¹: {embedding_config.get('apiEndpoint', 'N/A')}")
        
        config = CodeIndexConfig(
            root_dir=str(repo_path.resolve()),
            db_path=str(db_path.resolve()),
            languages=languages,
            embedding_options=embedding_config,
        )
        
        with CodeIndexClient(config) as client:
            for i, query in enumerate(queries, 1):
                try:
                    print(f"   [{i}/{len(queries)}] æœç´¢: '{query}'...", end=" ", flush=True)
                    results = client.semantic_search(
                        query=query,
                        topK=5,
                        minSimilarity=0.6,
                        embeddingOptions=embedding_config,  # ä¼ é€’ embeddingOptions ä»¥åˆå§‹åŒ– EmbeddingGenerator
                    )
                    print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                    
                    for result in results:
                        symbol = result.get("symbol", {})
                        location = result.get("location", {})
                        
                        code_snippets.append({
                            "query": query,
                            "symbol_name": symbol.get("qualifiedName", ""),
                            "symbol_kind": symbol.get("kind", ""),
                            "file_path": location.get("path", ""),
                            "line_number": location.get("startLine", 0),
                            "summary": symbol.get("chunkSummary", ""),
                            "similarity": result.get("similarity", 0.0),
                        })
                except Exception as e:
                    print(f"å¤±è´¥: {e}")
                    # Continue with next query if one fails
                    continue
        
        # Deduplicate by symbol name and file path
        seen = set()
        unique_snippets = []
        for snippet in code_snippets:
            key = (snippet["symbol_name"], snippet["file_path"])
            if key not in seen:
                seen.add(key)
                unique_snippets.append(snippet)
        
        print(f"   å»é‡åå‰©ä½™ {len(unique_snippets)} ä¸ªå”¯ä¸€ç»“æœ")
        return unique_snippets[:20]  # Limit to top 20 unique results
        
    except Exception as e:
        print(f"   âŒ CodeIndex æœç´¢å¤±è´¥: {e}")
        print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        # If codeindex fails, return empty results
        return code_snippets


def _get_embedding_config(
    llm_api_key: Optional[str] = None,
    llm_api_base_url: Optional[str] = None,
    embedding_model: Optional[str] = None,
) -> Dict[str, Any]:
    """Get embedding configuration from parameters, config file, or environment variables.
    
    Priority order:
    1. Function parameters (highest priority)
    2. Config file (autospecman.toml)
    3. Environment variables (lowest priority)
    
    Args:
        llm_api_key: API key from function parameter (highest priority)
        llm_api_base_url: API base URL from function parameter (highest priority)
        embedding_model: Embedding model from function parameter (highest priority)
    
    Returns:
        Dictionary with embedding configuration, or empty dict if no API key found
    """
    # Priority: parameter > config file > environment variable
    # Note: config file is loaded in the caller and passed as parameters
    api_key = (
        llm_api_key
        or os.getenv("OPENAI_API_KEY")
        or os.getenv("LLM_API_KEY")
    )
    
    api_base_url = (
        llm_api_base_url
        or os.getenv("LLM_API_BASE_URL")
        or os.getenv("OPENAI_API_BASE_URL")
        or "https://api.openai.com/v1"
    )
    
    # Embedding model: parameter > config file > environment variable > default
    model = (
        embedding_model
        or os.getenv("EMBEDDING_MODEL")
        or "text-embedding-3-small"
    )
    
    if not api_key:
        return {}
    
    # CodeIndex expects apiEndpoint to be the full embeddings endpoint URL
    api_endpoint = api_base_url.rstrip("/") + "/embeddings"
    
    return {
        "apiEndpoint": api_endpoint,
        "apiKey": api_key,
        "model": model,
    }


def _analyze_error_handling_with_llm(
    analyzer: LLMAnalyzer,
    code_snippets: List[Dict[str, Any]],
) -> Optional[Dict[str, Any]]:
    """Analyze error handling patterns using LLM."""
    # Prepare code snippets summary for LLM
    snippets_text = []
    for i, snippet in enumerate(code_snippets[:15], 1):  # Limit to 15 for prompt size
        snippets_text.append(
            f"{i}. {snippet['symbol_name']} ({snippet['symbol_kind']}) "
            f"in {snippet['file_path']}:{snippet['line_number']}\n"
            f"   Summary: {snippet.get('summary', 'N/A')[:200]}"
        )
    
    prompt = f"""åˆ†æä»¥ä¸‹ä»£ç ç‰‡æ®µï¼Œæ¨æ–­è¯¥æ¡†æ¶çš„é”™è¯¯å¤„ç†æ–¹å¼ï¼š

æ‰¾åˆ°çš„ç›¸å…³ä»£ç ç‰‡æ®µï¼š
{chr(10).join(snippets_text)}

è¯·åˆ†æå¹¶è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{{
  "approach": "é”™è¯¯å¤„ç†æ–¹å¼çš„ç®€è¦æè¿°ï¼Œä¾‹å¦‚ 'ä½¿ç”¨è‡ªå®šä¹‰å¼‚å¸¸ç±»'ã€'ä½¿ç”¨é”™è¯¯ç æšä¸¾'ã€'ä½¿ç”¨æ ‡å‡†å¼‚å¸¸åº“'ã€'ä½¿ç”¨ Result ç±»å‹' ç­‰",
  "details": "è¯¦ç»†çš„é”™è¯¯å¤„ç†æœºåˆ¶è¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š1) å¼‚å¸¸/é”™è¯¯ç±»å‹å®šä¹‰æ–¹å¼ 2) é”™è¯¯ä¼ æ’­æœºåˆ¶ 3) é”™è¯¯å“åº”æ ¼å¼ 4) é”™è¯¯ç ä½“ç³»ï¼ˆå¦‚æœæœ‰ï¼‰"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""
    
    print("   æ­£åœ¨è°ƒç”¨ LLM API...", end=" ", flush=True)
    try:
        # Build custom payload for error handling analysis
        url = f"{analyzer.api_base_url}/chat/completions"
        payload = {
            "model": analyzer.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a code analysis expert. Analyze error handling patterns and return JSON only.",
                },
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.3,
            "response_format": {"type": "json_object"},
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {analyzer.api_key}",
        }
        
        # Use the same HTTP call method as LLMAnalyzer
        if hasattr(analyzer, "_call_with_requests") and callable(analyzer._call_with_requests):
            response_data = analyzer._call_with_requests(url, payload, headers)
        else:
            response_data = analyzer._call_with_urllib(url, payload, headers)
        
        content = response_data.get("choices", [{}])[0].get("message", {}).get("content")
        if not content:
            print("å¤±è´¥: å“åº”ä¸ºç©º")
            return None
        
        # Parse the JSON response
        result = json.loads(content)
        print("æˆåŠŸ")
        
        # Extract approach and details
        return {
            "approach": result.get("approach"),
            "details": result.get("details"),
        }
    except (json.JSONDecodeError, KeyError, AttributeError, Exception) as e:
        print(f"å¤±è´¥: {e}")
        print(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return None


def _basic_error_handling_analysis(
    code_snippets: List[Dict[str, Any]],
) -> str:
    """Basic error handling analysis without LLM."""
    if not code_snippets:
        return "æœªæ£€æµ‹åˆ°æ˜ç¡®çš„é”™è¯¯å¤„ç†æœºåˆ¶"
    
    # Count symbol kinds
    kind_counts = {}
    for snippet in code_snippets:
        kind = snippet.get("symbol_kind", "")
        kind_counts[kind] = kind_counts.get(kind, 0) + 1
    
    # Infer approach from symbol kinds
    if any("exception" in snippet["symbol_name"].lower() for snippet in code_snippets):
        return "ä½¿ç”¨å¼‚å¸¸ç±»è¿›è¡Œé”™è¯¯å¤„ç†"
    if any("error" in snippet["symbol_name"].lower() for snippet in code_snippets):
        return "ä½¿ç”¨é”™è¯¯ç±»å‹è¿›è¡Œé”™è¯¯å¤„ç†"
    if any("result" in snippet["symbol_name"].lower() for snippet in code_snippets):
        return "ä½¿ç”¨ Result ç±»å‹è¿›è¡Œé”™è¯¯å¤„ç†"
    
    return f"æ£€æµ‹åˆ° {len(code_snippets)} ä¸ªç›¸å…³ä»£ç ç‰‡æ®µï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ"

